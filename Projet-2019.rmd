---
title: "Mélange de Bernoulli"
author: "Shi de Milleville Guillaume, Durand Lénaïc"
date: "24/10/2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Modèle


Considérons un vecteur aléatoire binaire $\boldsymbol{x} \in [0,1]^p$ de $p$ variables $x_j$ suivant chacune
une distribution de Bernoulli $\mathcal{B}(\mu_j)$. La distribution du vecteur s'exprime comme:
$$
p(\boldsymbol{x}| \boldsymbol{\mu}) = \prod_{j=1}^p \mu_j^{x_j} (1-\mu_j)^{1-x_j}, 
$$
avec $\boldsymbol{x}=(x_1, \cdots, x_p)^T$ et  $\boldsymbol{\mu}=(\mu_1, \cdots, \mu_p)^T$.

Soit une distribution  mélange à $K$ composantes  de Bernoulli
$$
p(\boldsymbol{x} | \boldsymbol{\pi}, \boldsymbol{M}) = \sum_{k=1}^K
                  \pi p(\boldsymbol{x} | \boldsymbol{\mu}_k)
$$
où les $\pi_k$ sont les proportions du mélange et les $p(\boldsymbol{x} | \boldsymbol{\mu}_k)$ sont des distributions de Bernoulli multivariées de
paramètres  $\boldsymbol{\mu}_k=(\mu_{k1}, \cdots, \mu_{kp})^T$, et $M=\{\boldsymbol{\mu}_1, \cdots , \boldsymbol{\mu}_K\}^T$
la matrice des paramètres des densités de classes.

Dans la suite nous considérerons
\begin{itemize}
\item un échantillon observé $X = \{\boldsymbol{x}_1, \cdots, \boldsymbol{x}_n\}$ issu de cette distribution mélange, 
\item des  variables latentes $Z=\{z_1, \cdots, z_n\}$ indiquant la composante d'origine de chaque $\boldsymbol{x}_i$.  
\end{itemize}

```{r , echo=FALSE,  warning=FALSE, error=FALSE, include=FALSE}
library(tidyverse)
library(reshape2)
```
  
## Simulation
```{r}
set.seed(3)
K<-3
p<-50
n<-200
pi<-c(1/3,1/3,1/3)
M<-matrix(runif(K*p),K,p)
M[K,]<-1-M[1,]
nks<-rmultinom(1,200,prob = pi)
Z<-rep(1:length(nks),nks)
X <-do.call(rbind, 
                  mapply(function(nk,k){
                    matrix(rbernoulli(nk*p,p=M[k,]),
                           nrow = nk,
                           ncol=p,
                           byrow = TRUE)}, nks,1:K))

kmeans(X,3,nstart = 10)->res.kmeans
tidyData<-melt(X[order(res.kmeans$cluster),order(M[1,])])

ggplot(tidyData, aes(x = Var2, y = Var1)) + 
  geom_raster(aes(fill=value)) + 
  scale_fill_brewer(aesthetics = "fill") + 
  labs(x="Variables", y="Individus", title="Matrix") 
```


## Exercice 2

### Question 1

On cherche à calculer $L(X,Z;\theta=\{\pi, M\})$

$L(X,Z;\theta=\{\pi, M\}) = \prod_{i=1}^n \mathbb P_\theta (X=x_i, Z = z_i)$

En utilisant la formule de Bayes, puis sachant que $\mathbb P (X = x_i | z_i = k) = \mathbb P(X=xi; \mu _ k)$ avec $k \in \{1, 2, 3\}$ 

On trouve : 


 $L(X,Z;\theta=\{\pi, M\}) =\prod_{i=1}^n \mathbb P_\theta (Z = z_i)\mathbb P_\theta (X = x_i|Z=z_i)$ 

$L(X,Z;\theta=\{\pi, M\}) = \frac {1}{3^n}\prod_{i=1}^n[\prod_{k=1}^K(\prod_{j=1}^p \mu_{k,j}^{x_{i,j}}(1-\mu_{k,j}^{1-x_{i,j}})]$ 

$\mathcal L(X, Z; \theta) = -n ln(3) + \sum_{i=1}^n[\sum_{k=1}^K(\sum_{j=1}^p[x_{i,j}ln(\mu_{k,j}) + (1-x_{i,j})ln(1-\mu_{k,j})])]$ 


### Question 2

$$
\begin{array}{l}{\text { On note } t_{i, k}^{q}=\mathbb{E}_{\theta^{q}}\left[Z_{i, k} | X_{i}\right]} \\ {t_{i, k}^{q}=p_{\theta^{q}}\left(Z_{i, k}=1 | X_{i}\right)} \\ {t_{i, k}^{q}=\frac{p_{\theta q}\left(X_{i} | Z_{i, k}\right) p\left(Z_{i, k}=1\right)}{\sum_{l=i}^{K} p_{\theta} q\left(X_{i} | Z_{l}\right) p_{\theta}\left(Z_{i, l}=1\right)}} \\ {t_{i, k}^{q}=\frac{\pi{k} \prod_{j=1}^{p} \mu_{k, j}^{x_{i, j}}\left(1-\mu_{k, j}\right)^{\left(1-x_{i, j}\right)}}{\sum_{l=1}^{K} \pi{l} \prod_{j=1}^{p} \mu_{l, j}^{x_{i, j}}\left(1-\mu_{l, j}\right)^{\left(1-x_{i, j}\right)}}}\end{array}
$$



### Question 3

Par indépendance de $z_i, \theta ^q$ on a :
\begin{align*}
\mathbb{Q}(\theta^{q}|\theta)&=\mathbb{E}_{\theta^{q}}[ln(p(X,Z|\theta = \{\pi , M\}))]\\
&= \sum_{i=1}^n \sum_{k=1} ^K\mathbb{E}_{\theta^{q}}[\mathbb 1 (z_i =k) [ln(\pi_k) + \sum_{j=1}^p x_{ij}. ln (M_{kj}) + \sum_{i=1}^p(1-x_{ij}) ln(1-M_{kj})]]
\end{align*}

D'où : 
$$
\mathbb{Q}(\theta^{q}|\theta) =
\sum_{i=1}^n \sum_{k=1} ^K\mathbb{E}_{\theta^{q}}[t_{ik}^q[ln(\pi _k) + \sum _{j=1}^p x_{ij} .ln (M_{kj}) + \sum _{i=1}^p(1-x_{ij}) ln(1-M_{kj})]]
$$


### Question 4


\begin{align*}
\frac{\partial Q(\theta ^q|\theta)}{\partial M_{kj}} &= \sum{i=1}^m t_{ik}[x_{ij}\frac{\partial ln(M_{kj})}{\partial M_{kj}}+(1-x_{ij})\frac{\partial ln(1-M_{kj})}{\partial M_{kj}}] \\
&= \frac{1}{M_{kj}(1-M_{kj})}(\sum_{i=1}^n t_{ik}^q[x_{ij}(1-M_{kj}) + (x_{ij}-1)M_{kj}]) \\
&= \frac{1}{M_{kj}(1-M_{kj})}\sum_{i=1}^n t_{ik}^q x_{ij} - M_{kj}\sum_{i=1}^n t_{ik}^q
\end{align*}

On trouve 0 si et seulement si 
$$
M_{kj} = \frac{\sum _{i=1}^n t_{ik}^q x_{ij}}{\sum _{i=1}^n t_{ik}^q}
$$

  \begin{align*}
  \frac{\partial Q(\theta ^q|\theta)}{\partial \pi} &= \sum_{k=1}^K(ln(\pi K\sum{i=1}^n t_{ik}^q))\\
  =& (\sum{i=1}^n t_{ik}^q) ln(\pi 1) + (\sum{i=1}^n t_{ik}^q) ln(\pi 2) + ... +(\sum{i=1}^n t_{ik}^q) ln(\pi K)\\
  =& \alpha_1 ln(\pi 1) + \alpha_2 ln(\pi 2) + ... + \alpha_K ln(\pi K)
  \end{align*}

En utilisant la méthode du Lagrangien on trouve 

$$
\pi_k = \frac{\sum _{i=1}^n t_{ik}^q}{\sum_{l=1}^K (\sum _{i=1}^n t_{il}^q)}
$$

### Question 5

Initialisation de $\theta ^{(0)}$ au hasard

On effectue les opérations suivantes tant que l'algorithme n'a pas convergé vers une solution : 

Calcul de l'espérance $\mathbb{Q}(\theta^{q}|\theta) = \mathbb E[L((X,Z),\theta)|\theta ^{q}]$

Maximisation tel que $\theta^{q+1}=\smash{\mathop{{\rm argmax}}\limits_{\theta}}\,(\mathbb{Q}(\theta^{q}/\theta))$

On augmente la valeur de q : $q = q+1$

Fin

La maximisation devrait faire tendre $\theta^{(q+1)}$ vers $L(X; \theta^{(c+1)})$

### Question 6

\begin{align*}
- \mathbb E[ln(p_{\theta ^{q+1}})(z|x)] &= - \mathbb E[ln(p_{\theta ^{q+1}})] \\
&= -\mathbb{E}\left[\ln p_{\theta+1}(z, x)\right]+\mathbb{E}\left[\ln p_{\theta+1}(x)\right]\\
=& -\sum_{i=1}^{n} \sum_{k=1}^{k}\left[t_{i k}^{q+1} \cdot\left(l_{k} \pi{k}+\sum_{j=1}^{p} x_{i j} \ln \eta_{k j}+\left(1-x_{i j}\right) h\left(1-M_{k j}\right)\right)\right]\\
&= -\mathbb{E}\left[\ln p_{\theta+1}\right]=\sum_{i=1}^{n} \sum_{k=1}^{k}\left(1-t_{i k}^{q+1}\right)\left(\ln \pi{k}+\sum_{j=1}^{p} x_{i,j} \cdot \ln M_{k j}+\left(1-x_{i,j}\right) \cdot \ln \left(1-M_{k j}\right)\right)
\end{align*}

### Question 7

$$
ln(p_{\bar \theta}(X))=\sum_{i=1}^{n} \sum_{k=1}^{k}\left[\ln \pi{k}+\sum_{j=1}^{p}\left(x_{i j} \ln M_{k j}+\left(1-x_{i j}\right) \cdot \ln \left(1-M_{k j}\right)\right)\right]
$$

### Question 8
$$
\hat K_{BIC} = argmax(ln \mathbb P_{\hat \theta K}(X)-\frac{d_K}{2}ln n  \\
B I C=\arg _{A} \max \left[\ln \mathbb{P}_{\hat{\theta} A}(x)\right]-\frac{K \cdot p+K-1}{2} \ln (n)
$$

### Question 9

D'après la question 6, 

$$
icL = BIC + \sum_{i=1}^{n} \sum_{k=1}^{K}(t_{ik}^{q+1}-1)(\ln \pi{k}+\sum_{j=1}^{l} x_{i,j}(ln M_{k j})+(1-x_{j}) \cdot \ln(1-M_{k j})
$$

### Question 10


Initialisation de la matrice M avec des $\mu_k, \pi_k$ M_step_M

Initialisation de la matrice Pi avec comme coefficients 1/3, 1/3, 1/3 M_step_pi

Calcul de Q(X,M,Pi) (incluant les calculs des $t_{ik}$)

Condition d'arrêt sur Q: Q(X,M,Pi)-Q(X,M_step_M, M_step_pi)<$\epsilon${

  on calcule les $t_{ik}$ pour E

  on calcule les $\pi_k$ et les $\mu_k$

  M = M_step_M

  Pi = M_step_pi

  M_step_M et M_step_pi prend les nouvelles valeurs de $\pi_k$ et $\mu_k$

}

return Q


### Question 11

On commence avec une partition $G^{(0)}$

$t \leftarrow 0$

TANT QUE $\mathcal L(\mathcal{C}, \pi^{(t+1)}, \Theta^{(t+1)}, G^{(t+1)}) - \mathcal L (\mathcal{C}, \pi^{(t)}, \Theta^{(t)}, G^{(t)}) > \epsilon$




  Etape E : Estimer avec $\{\pi^{(t)}, \theta ^{(t)}\}$ les probabilités d'appartenance aux $G_k$ pour $k \in [|1,K|]$ :
  
$$
\forall d \in \mathcal{C} ; \ln P(y=k | \mathbf{d}) \propto \ln \pi^{(t)}+\sum_{j=1}^{V} \mathfrak{tf}_{j, d_{i}} \ln \theta_{j|k}^{(t)}
$$


  Etape C : Assigner à chaque exemple $e_i$ une partition, celle dont le log de la probabilité est maximale. Noter $G^{(t+1)}$ la nouvelle partition

  Etape M : Estimation des nouveaux $\{\pi^{(t+1)}, \theta ^{(t+1)}\}$ avec

$$
\begin{aligned} \forall j, \forall k, \theta_{j | k}=& \frac{\sum_{i=1}^{N} t_{k i} \mathrm{t} f_{j, d_{i}}}{\sum_{i=1}^{N} t_{k i} \sum_{j=1}^{V} \mathrm{tf}_{j, d_{i}}} \\  \forall k, \pi{k}=& \frac{\sum_{i=1}^{N} t_{k i}}{N} \end{aligned}
$$

$t\leftarrow t +1$

Fin tant que



## Exercice 3

### Question 1

On vérifie que la sortie de l'algorithme E_step est cohérente. La somme des colonnes fait bien 1.

```{r}
pi2 <- c(1/3,1/3,1/3)
M2 <- t(matrix(c(rep(0.45,50), rep(0.5, 50), rep(0.55,50)), nrow=50, ncol=3)) # pris au hasard

E_step <- function(X, pi, M){ #calcul des tik conformément à ce qui est écrit ci dessus
  p <- length(M[1,])
  K <- length(M[,1])
  n <- length(X[,1])
  res <- matrix(0, K, n)
  for (i in (1:n)){
    den <- 0
    for(j in (1:K)){
      den <- den + pi[j]*prod((M[j,])^(X[i,])*(1-M[j,])^(1-X[i,]))
    }
  num <- 0
  for (k in (1:K)){
    num <- pi[k]*prod((M[k,])^(X[i,])*(1-M[k,])^(1-X[i,]))
    res[k,i] <- num/den
  }
  }
  return(res)
}

tik2estime <- E_step(X,pi,M)
tik2estime [1:3,1:3]
# tik2estime # mais ça ajoute 3 pages si on print
```

On trouve à chaque fois une valeur très proche de 1 et les deux autres très proches de 0 mais ceci est justifié puisqu'on utilise M.

### Question 2

Recherche du $\theta$ maximisant la vraisemblance (l'efficacité de l'algorithme est montré à la question 4)
On affiche M.

```{r}
M_step_M <- function(X, pi2, m2, epsilon){
  N <- length(X[,1])
  K <- length(M2[,1])
  p <- length(M2[1,])
  set.seed(42)
  M3 <- matrix(rep(0,K*p), K,p)
  tik2 <- E_step(X, pi2, M2)
  for(k in (1:K)){
    for (i in (1:p)){
      num <- 0 
      den <- 0
      for (j in (1:N)){
        num <- num + tik2[k,j]*X[j,i]
        den <- den + tik2[k,j]
      }
      M3[k,i] <- max(epsilon, min(num/den, 1-epsilon))
    }
  }
  return(M3)
}

M_step_pi <- function(X, pi2, M2){
  K <- length(M2[,1])
  M_step_pi <- rep(0,K)
  N <- length(X[,1])
  tik2 <- E_step(X, pi2, M2)
  for(k in (1:K))
  {
    for(i in (1:N))
    {
      M_step_pi[k]<-M_step_pi[k]+tik2[k,i]
    }
    M_step_pi[k]<-M_step_pi[k]/N
  }
  return(M_step_pi)
} 


Q<-function(X,pi,M){
    K<-length(M[,1])
    p<-length(X[1,])
    N<-length(X[,1])

    tik2<-E_step(X,pi,M)
    
    res<-0
    
    for(i in (1:N)){
      for(k in (1:K)){
        res<-res+log(pi[k])*tik2[k,i]
        for(j in (1:p)){
          res<-res+tik2[k,i]*(X[i,j]*log(M[k,j])+(1-X[i,j])*log(1-M[k,j]))
        }
      }
      
    }
    return(res)
}

M_step_M(X,pi,M,10e-2)
```



On affiche $\pi$.

```{r}
M_step_pi(X,pi,M)
```


### Question 3

Soit 

$$
||\Theta^{(q)}-\Theta^{(q-1)}|| =  \frac{p}{p+Kp} (||\pi^{q}-\pi^{q-1}|| + K*||M^{q}-M^{q-1}||)
$$

La condition d'arrêt de l'algorithme EM est  $||\Theta^{(q)}-\Theta^{(q-1)}||> \epsilon$


```{r}
class_Zi <- function(X, pi, M2, K)
{
  N <- length(X[,1])
  P <- length(X[1,])
  K <- length(M2[,1])
  
  Z <- rep(0,N)
  
  for(i in (1:K))
  {
   Z[i] <- which.max(c(E_step(X, pi, M)))
  }
  
}


EM<-function(X,K){
  margin<-10e-3
  epsilon<-10e-3
  p<-length(X[1,])
  N<-length(X[,1])
  M2<-matrix(runif(K*p),K,p)
  pipi<-c()
  for(j in (1:K))
  {
    pipi<-c(pipi,1/K)
  }
  Z2<-class_Zi(X,pipi,M2)
  norm_<-c()
  Q2<-c(Q(X,pipi,M2))
  dis<-Inf
  while(dis>epsilon)
  {
    new_M2<-M_step_M(X,pipi,M2,margin)
    new_pipi<-M_step_pi(X,pipi,M2)
    Z2<-class_Zi(X,pipi,M2)
    dis<-norm(new_M2-M2)*(K*p/(K+K*p))+norm(as.matrix(new_pipi-pipi))*(K/(K+K*p))
    norm_<-c(norm_,dis)
    M2<-new_M2
    pipi<-new_pipi
    
    Q2<-c(Q2,Q(X,pipi,M2))
  }
  
  
  ICL2<-ICL(X,pipi,M2)
  BIC2<-BIC(X,pipi,M2)
  
  return(list(M=M2,Z=Z2,pi=pipi,norm=norm_,Q=Q2,BIC=BIC2,ICL=ICL2))
}


BIC<-function(X,pipi,M2)
{
    N<-length(X[,1])
    K<-length(M2[,1])
    p<-length(X[1,])
    tik2<-E_step(X,pipi,M2)
    BIC<-0
    for(i in (1:N)){
      for(k in (1:K)){
        for(j in (1:p)){
          BIC<-BIC+tik2[k,i]*(X[i,j]*log(M2[k,j])+(1-X[i,j])*log(1-M2[k,j]))
        }
      }
    }
    BIC<-2*BIC-((p+1)*K-1)*log(N)
  return(BIC)
}

ICL<-function(X,pipi,M2){
  N<-length(X[,1])
  K<-length(M2[,1])
  p<-length(X[1,])
  return(Q(X,pipi,M2)-((p+1)*K-1)/2*log(N))
}


result_EM<-EM(X,3)
result_EM$M

M_temp=result_EM$M

# M_temp[1,]
# summary(M_temp[1,])

nb_itteration<-(1:length(result_EM$BIC_evo))

```

Pour vérifier la convergence de la matrice on essaie de recalculer cette matrice en changeant l'entier dans set.seed(int). On remarque que les résultats convergent bien vers une même solution (paramètres d'un mélange de Bernoulli en K classes avec K=3 ici)

### Question 4

```{r}
nb_itteration<-(1:length(result_EM$Q))
plot(main="évolution de la vraissemblance par étape",nb_itteration,result_EM$Q)
```

On a bien une vraissemblance qui converge vers un maximum de façon logarithmique lorsque l'algorithme est appliqué aux données simulées.

### Question 5

Calcul du BIC

```{r}
nb_itteration<-(1:length(result_EM$BIC))
print(main = " BIC :",result_EM$BIC)

```

### Question 6

Calcul du ICL

```{r}
nb_itteration<-(1:length(result_EM$ICL))
print(main="ICL  :",result_EM$ICL)
```

L'ICL et le BIC ont des valeurs négatives ce qui s'explique par leur formule (log d'un nombre proche de 0)


## Exercice 4


```{r}
K=10
nlignes = 5
ncolonnes = 133
epsilon = 10e-3
yac <- read.csv("raw_data.csv")
X4 <- yac[2:ncolonnes+2]

pi4 = c(0)
M4 <- matrix(0, nlignes, ncolonnes)
#initialisation aléatoire

for (i in (1:ncolonnes)){
  pi4 <- c(pi4, c(rep(i/ncolonnes)))

}

for (j in (1:nlignes)){
  M4[j,] <- j/(nlignes+1)
}
res <- E_step(X4, pi4, M4)
res[1:5,1:5]


# appliquer la méthode M





# resM <  - M_step_M(X4, pi4, m4, epsilon)
# resM
# M_step_pi(X, pi2, M2)
# EM(X4, M4)

```

On affiche les résultats de E_step appliqué à un échantillon de données pour vérifier que la somme des colonnes fait 1.
L'étape M ne fonctionnant pas pour ce jeu de données (problème de dimensions avec K inconnu), seul un échantillon de données de sortie de l'étape E est ici fourni
